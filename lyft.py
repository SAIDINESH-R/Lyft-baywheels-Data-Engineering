# -*- coding: utf-8 -*-
"""lyft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lPn1653E8GCkyO1xIg5KvS_yptclghTz
"""

import pandas as pd

df = pd.read_csv('/content/202411-baywheels-tripdata.csv')

df.head()

"""Dimensional Tables"""

import pandas as pd
import numpy as np
# Create date dimension from start and end dates
date_dim = df[['started_at', 'ended_at']].drop_duplicates().reset_index(drop=True)

# Convert columns to datetime
date_dim['started_at'] = pd.to_datetime(date_dim['started_at'])
date_dim['ended_at'] = pd.to_datetime(date_dim['ended_at'])

# Add date attributes
date_dim['date_key'] = range(1, len(date_dim) + 1)
date_dim['pick_hour'] = date_dim['started_at'].dt.hour
date_dim['pick_day'] = date_dim['started_at'].dt.day
date_dim['pick_month'] = date_dim['started_at'].dt.month
date_dim['pick_year'] = date_dim['started_at'].dt.year
date_dim['pick_weekday'] = date_dim['started_at'].dt.weekday
date_dim['pick_dayname'] = date_dim['started_at'].dt.day_name()
date_dim['drop_hour'] = date_dim['ended_at'].dt.hour
date_dim['drop_day'] = date_dim['ended_at'].dt.day
date_dim['drop_month'] = date_dim['ended_at'].dt.month
date_dim['drop_year'] = date_dim['ended_at'].dt.year
date_dim['drop_weekday'] = date_dim['ended_at'].dt.weekday
date_dim['drop_dayname'] = date_dim['ended_at'].dt.day_name()


# First convert datetime columns in both dataframes
df['started_at'] = pd.to_datetime(df['started_at'])
df['ended_at'] = pd.to_datetime(df['ended_at'])
date_dim['started_at'] = pd.to_datetime(date_dim['started_at'])
date_dim['ended_at'] = pd.to_datetime(date_dim['ended_at'])


# Add derived columns
date_dim['is_weekend'] = date_dim['pick_weekday'].isin([5, 6])
date_dim['is_peak_hour'] = date_dim['pick_hour'].isin([7, 8, 9, 16, 17, 18])

# Print preview of the date dimension
print("\nDATE DIMENSION PREVIEW")
print("=====================")
print(date_dim.head())

# 2. Create Start Station Dimension
start_stations = df[['start_station_id', 'start_station_name', 'start_lat', 'start_lng']].drop_duplicates()
start_stations = start_stations.rename(columns={
    'start_lat': 'start_latitude',
    'start_lng': 'start_longitude'
})
start_stations['start_station_key'] = np.arange(len(start_stations))

# Print preview of the station dimension
print("\nSTART STATION DIMENSION PREVIEW")
print("========================")
print(start_stations.head())

# 3. Create End Station Dimension
end_stations = df[['end_station_id', 'end_station_name', 'end_lat', 'end_lng']].drop_duplicates()
end_stations = end_stations.rename(columns={
    'end_lat': 'end_latitude',
    'end_lng': 'end_longitude'
})
end_stations['end_station_key'] = np.arange(len(end_stations))

# Print preview of the station dimension
print("\nEND STATION DIMENSION PREVIEW")
print("========================")
print(end_stations.head())

# 4. Create Bike Dimension
bike_dim = df[['ride_id', 'rideable_type']].drop_duplicates()
bike_dim['bike_key'] = np.arange(len(bike_dim))
bike_dim['is_electric'] = bike_dim['rideable_type'] == 'electric_bike'
bike_dim['bike_category'] = bike_dim['rideable_type'].map({
    'electric_bike': 'electric',
    'classic_bike': 'classic',
})

# Print preview of the bike dimension
print("\nBIKE DIMENSION PREVIEW")
print("=====================")
print(bike_dim.head())

# 5. Create Rider Dimension
rider_dim = df[['member_casual']].drop_duplicates()
rider_dim['rider_key'] = np.arange(len(rider_dim))
rider_dim['is_member'] = rider_dim['member_casual'] == 'member'
rider_dim['rider_type'] = rider_dim['member_casual'].map({
    'member': 'subscriber',
    'casual': 'casual'
})

# Print the first few rows
print(rider_dim.head())

# 6. Create Distance Dimension
# First create the distance category column
distance_ranges = [0, 1, 2, 5, 10, float('inf')]
distance_labels = ['Very Short', 'Short', 'Medium', 'Long', 'Very Long']

# Calculate distance in km using the latitude and longitude
df['distance_km'] = np.sqrt(
    (df['end_lat'] - df['start_lat'])**2 +
    (df['end_lng'] - df['start_lng'])**2
) * 111  # Rough conversion to kilometers

# Create distance categories
df['distance_category'] = pd.cut(
    df['distance_km'],
    bins=distance_ranges,
    labels=distance_labels
)

# Now create the distance dimension
distance_dim = df[['distance_category', 'distance_km']].drop_duplicates()
distance_dim['distance_key'] = np.arange(len(distance_dim))

# Verify the dimension was created
print("Distance Dimension Preview:")
print(distance_dim.head())
print("\nShape:", distance_dim.shape)

"""Fact Table"""

print("DataFrame shapes:")
print("Main df:", df.shape)
print("Date dimension:", date_dim.shape)
print("Start stations:", start_stations.shape)
print("End stations:", end_stations.shape)
print("Bike dimension:", bike_dim.shape)
print("Rider dimension:", rider_dim.shape)
print("Distance dimension:", distance_dim.shape)

# Also check column types
print("\nMain df dtypes:")
print(df.dtypes)

fact_table = df.copy()

# Calculate ride duration in minutes
fact_table['ride_duration_minutes'] = (fact_table['ended_at'] - fact_table['started_at']).dt.total_seconds() / 60

# Merge with date dimension
fact_table = fact_table.merge(
    date_dim[['date_key', 'started_at', 'ended_at']],
    on=['started_at', 'ended_at'],
    how='left'
)

# Merge with start station dimension
fact_table = fact_table.merge(
    start_stations[['start_station_key', 'start_station_id', 'start_station_name',
                   'start_latitude', 'start_longitude']],
    left_on=['start_station_id', 'start_station_name', 'start_lat', 'start_lng'],
    right_on=['start_station_id', 'start_station_name', 'start_latitude', 'start_longitude'],
    how='left'
)

# Merge with end station dimension
fact_table = fact_table.merge(
    end_stations[['end_station_key', 'end_station_id', 'end_station_name',
                 'end_latitude', 'end_longitude']],
    left_on=['end_station_id', 'end_station_name', 'end_lat', 'end_lng'],
    right_on=['end_station_id', 'end_station_name', 'end_latitude', 'end_longitude'],
    how='left'
)

# Merge with bike dimension
fact_table = fact_table.merge(
    bike_dim[['bike_key', 'ride_id', 'rideable_type']],
    on=['ride_id', 'rideable_type'],
    how='left'
)

# Merge with rider dimension
fact_table = fact_table.merge(
    rider_dim[['rider_key', 'member_casual']],
    on=['member_casual'],
    how='left'
)

# Merge with distance dimension
fact_table = fact_table.merge(
    distance_dim[['distance_key', 'distance_km']],
    on=['distance_km'],
    how='left'
)

# Drop the redundant columns
columns_to_drop = [
    'start_lat', 'start_lng', 'end_lat', 'end_lng',
    'start_latitude', 'start_longitude', 'end_latitude', 'end_longitude'
]
fact_table = fact_table.drop(columns=columns_to_drop, errors='ignore')

# Select only the necessary columns for the fact table
fact_table = fact_table[[
    'ride_id',
    'date_key',
    'start_station_key',
    'end_station_key',
    'bike_key',
    'rider_key',
    'distance_key',
    'ride_duration_minutes',
    'distance_km'
]]

# Verify no missing values in key columns
print("\nMissing values in fact table:")
print(fact_table.isnull().sum())
fact_table.head()

fact_table.shape

{"date_dim":date_dim.to_dict(orient="dict"),
    "start_stations":start_stations.to_dict(orient="dict"),
    "end_stations":end_stations.to_dict(orient="dict"),
    "bike_dim":bike_dim.to_dict(orient="dict"),
    "rider_dim":rider_dim.to_dict(orient="dict"),
    "distance_dim":distance_dim.to_dict(orient="dict"),
    "fact_table":fact_table.to_dict(orient="dict")}

"""# **GOOGLE BIG QUEREY**"""

from google.colab import auth
auth.authenticate_user()

# Also set up your project explicitly
from google.cloud import bigquery
import pandas as pd

# Set your project ID explicitly
project_id = 'my-lyft-project-444506'
client = bigquery.Client(project=project_id)

def upload_single_table(table_name, table_data):
    try:
        # Convert dict to dataframe
        df = pd.DataFrame(table_data)

        # Define table reference
        table_ref = f"{project_id}.lyft_data_engineering.{table_name}"

        print(f"Attempting to upload {table_name} to {table_ref}")

        # Upload configuration
        job_config = bigquery.LoadJobConfig(
            write_disposition="WRITE_TRUNCATE"
        )

        # Upload to BigQuery
        job = client.load_table_from_dataframe(
            df,
            table_ref,
            job_config=job_config
        )

        # Wait for the job to complete
        job.result()
        print(f"Successfully uploaded {table_name}")

    except Exception as e:
        print(f"Detailed error for {table_name}:")
        print(f"Error type: {type(e).__name__}")
        print(f"Error message: {str(e)}")

# Test with date_dim first
upload_single_table('date_dim', date_dim.to_dict(orient="dict"))

# Dictionary of all your tables
tables = {
    'date_dim': date_dim.to_dict(orient="dict"),
    'start_stations': start_stations.to_dict(orient="dict"),
    'end_stations': end_stations.to_dict(orient="dict"),
    'bike_dim': bike_dim.to_dict(orient="dict"),
    'rider_dim': rider_dim.to_dict(orient="dict"),
    'distance_dim': distance_dim.to_dict(orient="dict"),
    'fact_table': fact_table.to_dict(orient="dict")
}

# Upload each table
for table_name, table_data in tables.items():
    upload_single_table(table_name, table_data)

# Test BigQuery connection
def test_connection():
    try:
        # List datasets in your project
        datasets = list(client.list_datasets())
        print("Connection successful!")
        print(f"Found {len(datasets)} datasets in project {project_id}")
        for dataset in datasets:
            print(f"- {dataset.dataset_id}")
    except Exception as e:
        print(f"Connection error: {str(e)}")

test_connection()